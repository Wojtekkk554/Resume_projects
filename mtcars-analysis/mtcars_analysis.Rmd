---
title: "Mtcars analysis"
author: "Wojciech Kolasa"
date: "2025-07-23"
output:
  pdf_document: default
  always_allow_html: default
---
always_allow_html: true
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)


data(mtcars)
library(HDclassif)
library(cluster)
library(randomForest)
library(moments)
library(outliers)
library(nortest)
library(silvermantest)
library(heatmaply)
library(cluster)
library(plotly)

```

The data comes from R's built-in dataset called `mtcars`. Each observation consists of 11 features describing various parameters of selected car models. The aim of this project is to perform both supervised and unsupervised classification and to describe each variable.

### Analysis Plan:
1. Presentation of summary statistics including histograms  
2. Univariate analysis of outliers and multimodality  
3. Correlation analysis and dimensionality reduction  
4. Clustering analysis  
5. Supervised classification using random forests  

```{r statsytyki sumaryczne}
cat('Basic statistical measures','\n')
cat('\n')
apply(mtcars[,], 2 ,summary)
cat('\n')
cat('Standard deviation','\n')
cat('\n')
apply(mtcars[,] , 2 , sd)
cat('\n')
cat('Skewness','\n')
cat('\n')
apply(mtcars[,] , 2 ,skewness)
cat('\n')
cat('Kurtosis','\n')
cat('\n')
apply(mtcars[,] , 2 ,kurtosis)
```

## Summary and Conclusions:
The variables `disp` and `hp` have the highest minimum values and also the highest mean and median. They also have high standard deviation values, indicating low concentration around the mean. The rest of the variables show low standard deviation, suggesting high concentration around the mean. All variables have kurtosis greater than 0, indicating leptokurtic distributions. The variable `carb` has the highest kurtosis and the strongest presence of extreme values.

```{r grubbstest}
for(i in 1:11) print(grubbs.test(mtcars[,i]))
```

## Conclusions:
For the variables `qsec` and `carb`, the Grubbs test p-value is less than 0.05, indicating the presence of outliers that must be considered during clustering analysis.

```{r histogramy}
for(i in 1:10) hist(mtcars[,i+1], main=colnames(mtcars)[i+1])
```

## Conclusions:
Histogram analysis suggests that normality should be tested for `hp` and `drat`, and multimodality for `disp` and `wt`.

## Normality Tests:

```{r Shapiro Wilk}
shapiro.test(mtcars$hp)
shapiro.test(mtcars$drat)
```

## Conclusions:
The hypothesis of normality for variables `hp` and `drat` is rejected.

## Multimodality Analysis:

```{r wielomodalność}
silverman.test(mtcars$disp,k=1)
silverman.test(mtcars$wt,k=1)
```

## Conclusions:
The distributions of `disp` and `wt` are not multimodal, as the significance level exceeds 5%.

# Correlation Analysis using Pearson's coefficient

```{r Pearson}
heatmaply_cor(cor(mtcars[,],method='pearson'))
```

## Conclusions:
Positive correlations are observed within the groups: (`wt`, `disp`, `cyl`, `hp`), (`gear`, `am`, `drat`), and (`vs`, `qsec`). A weaker positive correlation is present between `vs` and `carb`.

## Correlation Analysis using Kendall's coefficient

```{r Kendall}
heatmaply_cor(cor(mtcars[,],method='kendall'))
```

## Conclusions:
The variables are less correlated in the same areas as with Pearson’s method.

## PCA

```{r pca}
prcomp(mtcars[,-1])->pca.mtcars
summary(pca.mtcars)
pca.mtcars$rotation[,1]
df.pca=data.frame(pc1=pca.mtcars$x[,1],pc2=pca.mtcars$x[,2],pc3=pca.mtcars$x[,3],kl=as.factor(mtcars$cyl))
plot_ly(df.pca,x=~pc1,y=~pc2,z=~pc3,color = ~kl,type='scatter3d') 
```

## Conclusions:
The first principal component explains 92.73% of the total variance. The variable `am` has the strongest influence on PC1. Clear cluster separation is visible along the PC1 axis.

## Unsupervised classification using k-means clustering

```{r kmeans}
kmeans(mtcars[,-1], centers=3)->km.mtcars.3
table(km.mtcars.3$cluster,mtcars$cyl)
df.km<-data.frame(x=pca.mtcars$x[,1],y=pca.mtcars$x[,2],z=pca.mtcars$x[,3],type=as.factor(km.mtcars.3$cluster))
plot_ly(df.km,x=~x,y=~y, z=~z,color=~type,type ='scatter3d')
```

## Conclusions:
A clear division into 3 clusters is visible, with one of the clusters being the most dispersed.

## Determining the optimal number of clusters

```{r ilość klastrow}
wss<-NA
for(i in 1:11) wss<-c(wss,kmeans(mtcars[,-1], centers=i)$tot.withinss)
plot(1:length(wss),wss)
```

## Conclusion:
Using the elbow method, it can be concluded that 3 or 4 clusters are most optimal.

```{r agne}
plot(agnes(dist(mtcars[,],method='minkowski',p=1),diss=T))
```

```{r diana}
plot(diana(mtcars[,]))
```

## Conclusions:
Both methods yield similar results. The coefficients differ by only 0.02.

### Supervised Classification
We use the random forest method.

```{r las losowy}
rfcv(mtcars[,-2],as.factor(mtcars$cyl))->rf.ic
rf.ic$error.cv
plot(rf.ic$n.var, rf.ic$error.cv, type="b", 
     xlab="Number of variables", ylab="Classification error", 
     main="Optimal number of variables")
```

## Conclusion:
The smallest cross-validation error occurs when the number of variables is between 2 and 5.

```{r ist cech}
randomForest(mtcars[,-2],as.factor(mtcars$cyl),importance = T)->rf.mtcars.imp
varImpPlot(rf.mtcars.imp)
```

## Conclusions:
According to the accuracy drop criterion, the most important variables are `disp`, `mpg`, `hp`, and `wt`. This number of variables is acceptable based on previous results.

```{r macierz}
randomForest(mtcars[,c(3,1,4,6)],as.factor(mtcars$cyl))->rf.mtcars.sel
rf.mtcars.sel$confusion
```

## Conclusion:
Using these variables, the maximum classification error is 28%.

## Final Conclusions
The variables `disp` and `hp` have high mean and standard deviation values. The Grubbs test indicated outliers in `qsec` and `carb`, which should be considered in cluster analysis. Histogram analysis helped identify variables for normality and multimodality testing. Normality tests showed that `hp` and `drat` do not follow a normal distribution, while Silverman’s test did not confirm multimodality for `disp` and `wt`. PCA analysis showed that the first principal component (PC1) explains 92.73% of the variance, mainly influenced by `am`. K-means clustering and the elbow method indicated 3 or 4 optimal clusters. Random forests are not an optimal classification model in this case, as the maximum classification error is 28%.


